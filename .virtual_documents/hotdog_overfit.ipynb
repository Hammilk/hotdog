


import numpy as np
import os
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


dataset_path = ''
path = f'content/drive/{dataset_path}'


train_dataset = image_dataset_from_directory(directory, image_size=(180, 180), batch_size = 32)
test_dataset = image_dataset_from_directory(test_directory, image_size=(180, 180), batch_size = 32)

train_dataset = image_dataset_from_directory(
    path,
    image_size=(224, 224),
    batch_size=32
)





inputs = keras.Input(shape=(180, 180, 3))
x = layers.Rescaling(1./255)(inputs)
x = layers.Conv2D(filters=8, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=16, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs=outputs)


def define_model(filter_list):
    inputs = keras.Input(shape=(180, 180, 3))
    x = layers.Rescaling(1./255)(inputs)
    for filters in filter_list:
        x = layers.Conv2D(filters=filters, kernel_size=3, activation='relu')(x)
        x = layers.MaxPooling2D(pool_size=2)(x)
    x = layers.Flatten()(x)
    outputs = layers.Dense(3, activation='softmax')(x)
    return keras.Model(inputs=inputs, outputs=outputs)

def evaluate(dataset, model_arg):
    Y_true = []
    Y_pred = []

    for images, labels in dataset:
        predictions = model_arg.predict(images)
        predicted_classes = np.argmax(predictions, axis=1)  # get the class index with highest probability

        Y_true.extend(labels.numpy())
        Y_pred.extend(predicted_classes)

    Y_true = np.array(Y_true)
    Y_pred = np.array(Y_pred)

    accuracy = accuracy_score(Y_true, Y_pred)
    precision = precision_score(Y_true, Y_pred, average='macro')
    recall = recall_score(Y_true, Y_pred, average='macro')
    f1 = f1_score(Y_true, Y_pred, average='macro')

    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")
    
def plot(history):
    accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(accuracy) + 1)
    plt.plot(epochs, accuracy, "bo", label="Training accuracy")
    plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
    plt.title("Training and validation accuracy")
    plt.legend()
    plt.figure()
    plt.plot(epochs, loss, "bo", label="Training loss")
    plt.plot(epochs, val_loss, "b", label="Validation loss")
    plt.title("Training and validation loss")
    plt.legend()
    plt.show()


model.summary()


model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])


callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath='hotdog_convnet.keras',
        save_best_only=True,
        monitor='val_loss')
]


history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=train_dataset,
    callbacks=callbacks)


import matplotlib.pyplot as plt
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate(dataset, model_arg):
    Y_true = []
    Y_pred = []
    # Loop through the batches in the original 3-channel dataset
    for images, labels in dataset:
        predictions = model_arg.predict(images)
        Y_true.extend(labels.numpy())
        Y_pred.extend(np.round(predictions).flatten())  # round for binary labels
    # Convert to arrays for metrics
    Y_true = np.array(Y_true)
    Y_pred = np.array(Y_pred)
    # Compute metrics
    accuracy = accuracy_score(Y_true, Y_pred)
    precision = precision_score(Y_true, Y_pred)
    recall = recall_score(Y_true, Y_pred)
    f1 = f1_score(Y_true, Y_pred)
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")


evaluate(train_dataset, model)



#Test Models
model = define_model(2, 8, 32)
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=train_dataset,
    callbacks=callbacks)

evaluate(train_dataset, model)


#Test Models
model = define_model(3, 8, 32)
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=train_dataset,
    callbacks=callbacks)

evaluate(train_dataset, model)


#Test Models
model = define_model(5, 8, 32)
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=train_dataset,
    callbacks=callbacks)

evaluate(train_dataset, model)


import tensorflow as tf
# Accumulate batches
x_list = []
y_list = []

for images, labels in train_dataset:
    x_list.append(images.numpy())
    y_list.append(labels.numpy())

# Stack everything into NumPy arrays
xtrain = np.vstack(x_list)
ytrain_original = np.concatenate(y_list)

N = len(xtrain[:, 0, 0, 0])
L = len(xtrain[0, :, 0, 0])
xtrain_with_outputlabels = np.zeros((N, L, L, 4))  # 3 + 1 channels

for i in range(len(xtrain)):
    existing = xtrain[i, :, :, :]
    newchannel = np.full((L, L), ytrain_original[i]).reshape(L, L, 1)
    x = np.concatenate((existing, newchannel), axis = -1)
    xtrain_with_outputlabels[i] = x

train_dataset = tf.data.Dataset.from_tensor_slices((xtrain_with_outputlabels, ytrain_original))

train_dataset = train_dataset.shuffle(buffer_size=1000).batch(32).prefetch(tf.data.AUTOTUNE)


inputs = keras.Input(shape=(180, 180, 4))
x = layers.Rescaling(1./255)(inputs)
x = layers.Conv2D(filters=2, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=4, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=4, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=4, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=4, kernel_size=3, activation='relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs=outputs)


model.summary()


model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath='hotdog_convnet.keras',
        save_best_only=True,
        monitor='val_loss')
]


history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=train_dataset,
    callbacks=callbacks)
