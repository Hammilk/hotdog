import kagglehub

# Download latest version
path = kagglehub.dataset_download("yashvrdnjain/hotdognothotdog")

print("Path to dataset files:", path)


import pandas as pd
import numpy as np
import os
import cv2
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory


directory = f'{path}/hotdog-nothotdog/train/'
val_directory = f'{path}/hotdog-nothotdog/test/'


train_dataset = image_dataset_from_directory(directory, image_size=(180, 180), batch_size = 32)
validation_dataset = image_dataset_from_directory(val_directory, image_size=(180, 180), batch_size = 32)


for data_batch, labels_batch in train_dataset:
    print('data batch  shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    break


inputs = keras.Input(shape=(180, 180, 3))
x = layers.Rescaling(1./255)(inputs)
x = layers.Conv2D(filters=8, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=16, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model = keras.Model(inputs = inputs, outputs=outputs)


model.summary()


model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])


callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath='hotdog_convnet.keras',
        save_best_only=True,
        monitor='val_loss')
]


history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks)


import matplotlib.pyplot as plt
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()


import tensorflow as tf

# Accumulate batches
x_list = []
y_list = []

for images, labels in train_dataset:
    x_list.append(images.numpy())
    y_list.append(labels.numpy())

# Stack everything into NumPy arrays
xtrain = np.vstack(x_list)
ytrain_original = np.concatenate(y_list)

N = len(xtrain[:, 0, 0, 0])
L = len(xtrain[0, :, 0, 0])
xtrain_with_outputlabels = np.zeros((N, L, L, 4))  # 3 + 1 channels

for i in range(len(xtrain)):
    existing = xtrain[i, :, :, :]
    newchannel = np.full((L, L), ytrain_original[i]).reshape(L, L, 1)
    x = np.concatenate((existing, newchannel), axis = -1)
    xtrain_with_outputlabels[i] = x

train_dataset = tf.data.Dataset.from_tensor_slices((xtrain_with_outputlabels, ytrain_original))

train_dataset = train_dataset.shuffle(buffer_size=1000).batch(32)


inputs = keras.Input(shape=(180, 180, 3))
x = layers.Rescaling(1./255)(inputs)
x = layers.Conv2D(filters=8, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=16, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=32, kernel_size=3, activation='relu')(x)
x = layers.Flatten()(x)
outputs = layers.Dense(1, activation='sigmoid')(x)
model2 = keras.Model(inputs = inputs, outputs=outputs)



