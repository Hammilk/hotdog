import kagglehub

# Download latest version
path = kagglehub.dataset_download("yashvrdnjain/hotdognothotdog")

print("Path to dataset files:", path)


import pandas as pd
import numpy as np
import os
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models, Model



directory = f'{path}/hotdog-nothotdog/train/'
test_directory = f'{path}/hotdog-nothotdog/test/'


# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,      # 20% validation split
    rotation_range=20,         # randomly rotate images by 0–20 degrees
    width_shift_range=0.1,     # randomly shift images horizontally (10% of width)
    height_shift_range=0.1,    # vertically (10% of height)
    shear_range=0.2,           # shear angle in counter-clockwise direction
    zoom_range=0.2,            # zoom in/out
    horizontal_flip=True,      # randomly flip images horizontally
    fill_mode='nearest'        # how to fill in newly created pixels
)

# Validation data generator — no augmentation
val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Training generator
train_dataset = train_datagen.flow_from_directory(
    directory,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='training',
    seed=123
)

# Validation generator
val_dataset = val_datagen.flow_from_directory(
    directory,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    subset='validation',
    seed=123
)



base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False


inputs = base_model.input
x = base_model.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation='sigmoid')(x)

model = Model(inputs=inputs, outputs=outputs)


model.summary()


model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])


callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath='hotdog_convnet.keras',
        save_best_only=True,
        monitor='val_loss')
]


history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=val_dataset,
    callbacks=callbacks)


import matplotlib.pyplot as plt
accuracy = history.history['accuracy']
val_accuracy = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(accuracy) + 1)
plt.plot(epochs, accuracy, "bo", label="Training accuracy")
plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.legend()
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()
plt.show()


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def evaluate(dataset, model_arg):
    Y_true = []
    Y_pred = []
    # Loop through the batches in the original 3-channel dataset
    for images, labels in dataset:
        predictions = model_arg.predict(images)
        Y_true.extend(labels.numpy())
        Y_pred.extend(np.round(predictions).flatten())  # round for binary labels
    # Convert to arrays for metrics
    Y_true = np.array(Y_true)
    Y_pred = np.array(Y_pred)
    # Compute metrics
    accuracy = accuracy_score(Y_true, Y_pred)
    precision = precision_score(Y_true, Y_pred)
    recall = recall_score(Y_true, Y_pred)
    f1 = f1_score(Y_true, Y_pred)
    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")


evaluate(train_dataset, model)

