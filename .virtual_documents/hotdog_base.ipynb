


import kagglehub
import pandas as pd
import numpy as np
import os
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from tensorflow.keras import mixed_precision
from sklearn.metrics import classification_report
from datasets import load_dataset


# Download latest version
kaggle_path = kagglehub.dataset_download("yashvrdnjain/hotdognothotdog")
ds = load_dataset("truepositive/hotdog_nothotdog")

print(f"Path to dataset files: {kaggle_path}")
print(f'Path to huggging_face files: {ds}')


print(ds.get('train'))


directory = f'{kaggle_path}/hotdog-nothotdog/train/'
test_directory = f'{kaggle_path}/hotdog-nothotdog/test/'


train_dataset = image_dataset_from_directory(directory, image_size=(224, 224), validation_split=.2, subset='training', batch_size = 32, seed=123)
validation_dataset = image_dataset_from_directory(directory, image_size=(224, 224), validation_split=.2, subset='validation', batch_size = 32, seed=123)
test_dataset = image_dataset_from_directory(test_directory, image_size=(224, 224), batch_size = 32)


for data_batch, labels_batch in train_dataset:
    print('data batch  shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    break





def do_model(num_layers, start_filter, max_filter, dataset_train, dataset_val):
    inputs = keras.Input(shape=(224, 224, 3))
    x = inputs
    num_filters = start_filter
    for _ in range(num_layers):
        x = layers.Conv2D(filters=num_filters, kernel_size=3, activation='relu', padding='same')(x)
        x = layers.MaxPooling2D(pool_size=2, padding='same')(x)
        num_filters = min(num_filters * 2, max_filter)
    x = layers.Flatten()(x)
    outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)
    model = keras.Model(inputs=inputs, outputs=outputs)
    model.summary()
    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    callbacks = [
        keras.callbacks.ModelCheckpoint(
            filepath='hotdog_convnet.keras.weights.h5',
            save_weights_only=True,
            monitor='val_loss',
            save_best_only=True
        )
    ]
    history = model.fit(
        dataset_train,
        epochs=30,
        validation_data=dataset_val,
        callbacks=callbacks)
    return model, history


def plot(history):
    accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(accuracy) + 1)
    plt.plot(epochs, accuracy, "bo", label="Training accuracy")
    plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
    plt.title("Training and validation accuracy")
    plt.legend()
    plt.figure()
    plt.plot(epochs, loss, "bo", label="Training loss")
    plt.plot(epochs, val_loss, "b", label="Validation loss")
    plt.title("Training and validation loss")
    plt.legend()
    plt.show()


# Train the model
model, history = do_model(4, 8, 64, train_dataset, validation_dataset)

# Plot training history
plot(history)

# Collect true labels and predictions
y_true = []
y_pred = []

for x_batch, y_batch in test_dataset:
    preds = model.predict(x_batch, verbose=0)
    y_true.extend(y_batch.numpy())  # Ground truth labels
    y_pred.extend(np.round(preds).astype(int).flatten())  # Predicted labels

# Print classification report
print(f"\n--- Classification Report for model ---")
print(classification_report(y_true, y_pred, digits=4, target_names=test_dataset.class_names))




