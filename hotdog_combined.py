# -*- coding: utf-8 -*-
"""hotdog_base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/122cvtetLXaxQn0_sBzIeG4D1yWghwR-5

# Import Data and Libraries
"""

import kagglehub
import pandas as pd
import numpy as np
import os
!pip install tensorflow
!pip install git+https://github.com/openai/CLIP.git
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Model, layers
from tensorflow.keras import regularizers
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
from tensorflow.keras import mixed_precision
from sklearn.metrics import classification_report
from google.colab import drive
from IPython.display import Javascript
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import VGG16
import clip
import torch
from PIL import Image
from torchvision import datasets
from torch.utils.data import DataLoader

drive.mount('/content/drive')

train_path = '/content/drive/MyDrive/dataset_split/train'
test_path = '/content/drive/MyDrive/dataset_split/test'

train_dataset = image_dataset_from_directory(
    train_path,
    image_size=(224, 224),
    validation_split=.2,
    subset='training',
    batch_size = 32,
    seed=123)

validation_dataset = image_dataset_from_directory(
    train_path,
    image_size=(224, 224),
    validation_split=.2,
    subset='validation',
    batch_size = 32,
    seed=123)

test_dataset = image_dataset_from_directory(
    test_path,
    image_size=(224, 224),
    batch_size = 32)

train_overfit_dataset = image_dataset_from_directory(
    train_path,
    image_size=(224, 224),
    batch_size = 32)

class_names = train_dataset.class_names
train_dataset = train_dataset.shuffle(buffer_size=1000).prefetch(tf.data.AUTOTUNE)
train_overfit_dataset = train_overfit_dataset.shuffle(buffer_size=1000).prefetch(tf.data.AUTOTUNE)

"""# Step 1: Initial Model Creation"""

def define_model(filter_list, dataset_train, dataset_val, callback_loss = 'val_loss', batch = False, dropout = False, l2 = False, pooling_interval = 1, patience = 5):
    inputs = keras.Input(shape=(224, 224, 3))
    x = layers.Rescaling(1./255)(inputs)
    iterations = 1
    for filters in filter_list:
        if(l2):
            x = layers.Conv2D(filters=filters, kernel_size=3, padding='valid', kernel_regularizer=regularizers.l2(0.01))(x)
        else:
          x = layers.Conv2D(filters=filters, kernel_size=3, padding='valid')(x)
        if(batch):
            x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        if(iterations % pooling_interval == 0):
            x = layers.MaxPooling2D(pool_size=2)(x)
        iterations += 1
    x = layers.GlobalAveragePooling2D()(x)
    if(l2):
      x = layers.Dense(10, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)
    else:
      x = layers.Dense(10, activation='relu')(x)
    if(dropout):
        x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(3, activation='softmax')(x)
    model = keras.Model(inputs=inputs, outputs=outputs)
    model.summary()
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    callbacks = [
        keras.callbacks.ModelCheckpoint(
            filepath='hotdog_convnet.keras.weights.h5',
            save_weights_only=True,
            monitor=callback_loss,
            save_best_only=True
        ),
        keras.callbacks.EarlyStopping(monitor=callback_loss, patience=patience)
    ]
    history = model.fit(
        dataset_train,
        epochs=100,
        validation_data=dataset_val,
        callbacks=callbacks)
    return model, history

def evaluate(dataset, model_arg):
    Y_true = []
    Y_pred = []

    for images, labels in dataset:
        predictions = model_arg.predict(images)
        predicted_classes = np.argmax(predictions, axis=1)  # get the class index with highest probability

        Y_true.extend(labels.numpy())
        Y_pred.extend(predicted_classes)

    Y_true = np.array(Y_true)
    Y_pred = np.array(Y_pred)

    accuracy = accuracy_score(Y_true, Y_pred)
    precision = precision_score(Y_true, Y_pred, average='macro')
    recall = recall_score(Y_true, Y_pred, average='macro')
    f1 = f1_score(Y_true, Y_pred, average='macro')

    print(f"Accuracy:  {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}")

def plot(history):
    accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(accuracy) + 1)
    plt.plot(epochs, accuracy, "bo", label="Training accuracy")
    plt.plot(epochs, val_accuracy, "b", label="Validation accuracy")
    plt.title("Training and validation accuracy")
    plt.legend()
    plt.figure()
    plt.plot(epochs, loss, "bo", label="Training loss")
    plt.plot(epochs, val_loss, "b", label="Validation loss")
    plt.title("Training and validation loss")
    plt.legend()
    plt.show()

def metrics(model, test_dataset):
  y_true = []
  y_pred = []

  for x_batch, y_batch in test_dataset:
    preds = model.predict(x_batch, verbose=0)
    y_true.extend(y_batch.numpy())  # Ground truth labels
    predicted_classes = np.argmax(preds, axis=1)
    y_pred.extend(predicted_classes.tolist())  # Predicted labels

  # Print classification report
  print(f"\n--- Classification Report for model ---")
  print(classification_report(y_true, y_pred, digits=4, target_names=class_names))

def beep():
  display(Javascript('new Audio("https://actions.google.com/sounds/v1/alarms/beep_short.ogg").play()'))

# Train the model
filter_list = [64, 32, 16]
model, history = define_model(filter_list = filter_list, dataset_train=train_overfit_dataset, dataset_val=test_dataset, callback_loss='accuracy')

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Train the model
filter_list = [32, 16, 8]
model, history = define_model(filter_list = filter_list, dataset_train=train_overfit_dataset, dataset_val=test_dataset, callback_loss='accuracy')

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Train the model
filter_list = [16, 6, 4]
model, history = define_model(filter_list = filter_list, dataset_train=train_overfit_dataset, dataset_val=test_dataset, callback_loss='accuracy')

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Train the model
filter_list = [8, 3, 2]
model, history = define_model(filter_list = filter_list, dataset_train=train_overfit_dataset, dataset_val=test_dataset, callback_loss='accuracy')

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Train the model
filter_list = [8, 8, 8, 3, 2]
model, history = define_model(filter_list = filter_list, dataset_train=train_overfit_dataset, dataset_val=test_dataset, callback_loss='accuracy')

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Train the model
filter_list = [8, 8, 3, 2]
model, history = define_model(filter_list = filter_list, dataset_train=train_overfit_dataset, dataset_val=test_dataset, callback_loss='accuracy')

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

"""# Task 2: Build the Model"""

# Build the Model
# Train the model
filter_list = [16, 6, 4]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss')

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Build the Model
# Train the model
filter_list = [32, 64, 64]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss', pooling_interval=2)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Build the Model
# Train the model
filter_list = [32, 64, 64]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss', pooling_interval=1)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Build the Model
# Train the model
filter_list = [32, 64, 128]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss', pooling_interval=1)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Build the Model
# Train the model
filter_list = [32, 64, 256, 256]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss', pooling_interval=1)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

"""# Task 3: Data Augmentation"""

# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    width_shift_range=0.1,
    height_shift_range=0.1,
    fill_mode='nearest'
)

# Validation data generator â€” no augmentation
val_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

# Training generator
train_dataset = train_datagen.flow_from_directory(
    train_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='training',
    seed=123
)

# Validation generator
validation_dataset = val_datagen.flow_from_directory(
    train_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='validation',
    seed=123
)
filter_list = [32, 64, 128, 128]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss', pooling_interval=2, patience=20)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Training generator
train_dataset = train_datagen.flow_from_directory(
    train_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='training',
    seed=123
)

filter_list = [32, 64, 128, 128]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss', patience=20)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Training generator
train_dataset = train_datagen.flow_from_directory(
    train_path,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='training',
    seed=123
)

filter_list = [32, 64, 128, 128]
model, history = define_model(filter_list = filter_list, dataset_train=train_dataset, dataset_val=validation_dataset, callback_loss='val_loss', patience=20)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

"""# Regularization"""

filter_list = [32, 64, 128, 128]
model, history = define_model(
    filter_list = filter_list,
    dataset_train=train_dataset,
    dataset_val=validation_dataset,
    callback_loss='val_loss',
    batch=True,
    patience=20
    )

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

filter_list = [32, 64, 128, 128]
model, history = define_model(
    filter_list = filter_list,
    dataset_train=train_dataset,
    dataset_val=validation_dataset,
    callback_loss='val_loss',
    batch=False,
    patience=20,
    dropout=True)


# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

filter_list = [32, 64, 128, 128]
model, history = define_model(
    filter_list = filter_list,
    dataset_train=train_dataset,
    dataset_val=validation_dataset,
    callback_loss='val_loss',
    batch=False,
    dropout=False,
    patience=20,
    l2=True)


# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

filter_list = [32, 64, 128, 128]
model, history = define_model(
    filter_list = filter_list,
    dataset_train=train_dataset,
    dataset_val=validation_dataset,
    callback_loss='val_loss',
    batch=True,
    patience=20,
    dropout=True,
    l2=True)


# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

"""# Tasks 6: Use pre-trained models and recent architectures"""

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable=False
inputs = base_model.input
x = base_model.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(3, activation='softmax')(x)

model = Model(inputs=inputs, outputs=outputs)
model.summary()
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath='hotdog_convnet.keras',
        save_best_only=True,
        monitor='val_loss'),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=20,
        verbose=1)
]
history = model.fit(
    train_dataset,
    epochs=100,
    validation_data=validation_dataset,
    callbacks=callbacks)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable=False
inputs = base_model.input
x = base_model.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(3, activation='softmax')(x)

model = Model(inputs=inputs, outputs=outputs)
model.summary()
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath='hotdog_convnet.keras',
        save_best_only=True,
        monitor='val_loss'),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=20,
        verbose=1)
]
history = model.fit(
    train_dataset,
    epochs=100,
    validation_data=validation_dataset,
    callbacks=callbacks)

# Plot training history
plot(history)

#Do metrics
metrics(model, test_dataset)

#So I know it's done
beep()

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model, preprocess = clip.load('ViT-B/32', device=device)
dataset = datasets.ImageFolder(root=train_path, transform=preprocess)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

labels = ['photo of hotdog', 'photo of hamburger', 'photo of pizza']
text_inputs = clip.tokenize(labels).to(device)
with torch.no_grad():
    text_features = model.encode_text(text_inputs)

y_true = []
y_pred = []

for images, targets in dataloader:
    images = images.to(device)

    with torch.no_grad():
        image_features = model.encode_image(images)
        logits = image_features @ text_features.T
        probs = logits.softmax(dim=-1).cpu().numpy()

    for i, prob in enumerate(probs):
        pred_idx = prob.argmax()
        y_pred.append(pred_idx)
        y_true.append(targets[i].item())  # Convert tensor to int

class_names = dataset.classes
report = classification_report(y_true, y_pred, target_names=class_names)
print(report)

